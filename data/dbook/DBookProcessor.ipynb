{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'original/'\n",
    "output_dir = './'\n",
    "melu_output_dir = '../../../MeLU/dbook/'\n",
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]\n",
    "\n",
    "if not os.path.exists(\"{}/meta_training/\".format(output_dir)):\n",
    "    os.mkdir(\"{}/log/\".format(output_dir))\n",
    "    for state in states:\n",
    "        os.mkdir(\"{}/{}/\".format(output_dir, state))\n",
    "        os.mkdir(\"{}/{}/\".format(melu_output_dir, state))\n",
    "        if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\n",
    "            os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ui_data = pd.read_csv(input_dir+'user_book.dat', names=['user','item','rating'], sep='\\t',engine='python')\n",
    "ul = pd.read_csv(input_dir+'user_location.dat', names=['user','location'], sep='\\t',engine='python')\n",
    "\n",
    "ba = pd.read_csv(input_dir+'book_author.dat', names=['book','author'], sep='\\t',engine='python')\n",
    "bp = pd.read_csv(input_dir+'book_publisher.dat', names=['book','publisher'], sep='\\t',engine='python')\n",
    "\n",
    "by = pd.read_csv(input_dir+'book_year.dat', names=['book','year'], sep='\\t',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22347"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(ui_data.item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10592, 20934)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list = list(set(ui_data.user) & set(ul.user))\n",
    "item_list = list(set(ui_data.item) & ((set(ba.book) & set(bp.book))) & set(by.book))\n",
    "len(user_list), len(item_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. user and item featur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 1698, 10544)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_list = list(set(ul[ul.user.isin(user_list)].location))\n",
    "publisher_list = list(set(bp[bp.book.isin(item_list)].publisher))\n",
    "author_list = list(set(ba[ba.book.isin(item_list)].author))\n",
    "len(location_list), len(publisher_list), len(author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10592/10592 [00:07<00:00, 1392.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10592"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "user_fea = {}\n",
    "for i in tqdm(user_list):\n",
    "    location_idx = location_list.index(list(ul[ul['user']==i].location)[0])\n",
    "    location = torch.tensor([[location_idx]]).long()\n",
    "    user_fea[i] = location\n",
    "len(user_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20934/20934 [00:35<00:00, 597.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20934, 20934)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_fea_homo = {}\n",
    "item_fea_hete = {}\n",
    "for i in tqdm(item_list):\n",
    "    publisher_idx = publisher_list.index(list(bp[bp['book']==i].publisher)[0])\n",
    "    publisher = torch.tensor([[publisher_idx]]).long()\n",
    "        \n",
    "    author_idx = author_list.index(list(ba[ba['book']==i].author)[0])\n",
    "    author = torch.tensor([[author_idx]]).long()\n",
    "    \n",
    "    item_fea_hete[i] = publisher\n",
    "    item_fea_homo[i] = torch.cat((publisher, author), 1)\n",
    "len(item_fea_hete), len(item_fea_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. mp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def reverse_dict(d):\n",
    "    # {1:[a,b,c], 2:[a,f,g],...}\n",
    "    re_d = collections.defaultdict(list)\n",
    "    for k, v_list in d.items():\n",
    "        for v in v_list:\n",
    "            re_d[v].append(k)\n",
    "    return dict(re_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20934, 10544)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_authors =  {k: g[\"author\"].tolist() for k,g in ba[ba.book.isin(item_list)].groupby(\"book\")}\n",
    "a_books = reverse_dict(b_authors)\n",
    "len(b_authors), len(a_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonKeys2int(x):\n",
    "    if isinstance(x, dict):\n",
    "            return {int(k):v for k,v in x.items()}\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4220it [00:00, 245457.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4220\n",
      "4220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4220"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 'meta_training'\n",
    "\n",
    "support_u_books = json.load(open(output_dir+state+'/support_u_books.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_books = json.load(open(output_dir+state+'/query_u_books.json','r'), object_hook=jsonKeys2int)\n",
    "support_u_books_y = json.load(open(output_dir+state+'/support_u_books_y.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_books_y = json.load(open(output_dir+state+'/query_u_books_y.json','r'), object_hook=jsonKeys2int)\n",
    "if support_u_books.keys() == query_u_books.keys():\n",
    "    u_id_list = support_u_books.keys()\n",
    "print(len(u_id_list))\n",
    "\n",
    "train_u_books = {}\n",
    "if support_u_books.keys() == query_u_books.keys():\n",
    "    u_id_list = support_u_books.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in tqdm(enumerate(u_id_list)):\n",
    "    train_u_books[int(u_id)] = []\n",
    "    train_u_books[int(u_id)] += support_u_books[u_id]+query_u_books[u_id]\n",
    "len(train_u_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4220"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_u_id_list = list(u_id_list).copy()\n",
    "len(train_u_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/4220 [00:00<00:47, 88.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4220/4220 [01:32<00:00, 45.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4220 4220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "u_b_u_books = {}\n",
    "u_b_a_books= {}\n",
    "\n",
    "support_b_users = reverse_dict(support_u_books)\n",
    "\n",
    "for u in tqdm(u_id_list):\n",
    "    u_b_u_books[u] = {}\n",
    "    u_b_a_books[u] = {}\n",
    "    for b in support_u_books[u]:\n",
    "        u_b_a_books[u][b] = set([b])\n",
    "        for _a in b_authors[b]:\n",
    "            cur_bs = a_books[_a]\n",
    "            u_b_a_books[u][b].update(cur_bs)\n",
    "        \n",
    "        u_b_u_books[u][b] = set([b])\n",
    "        u_b_u_books[u][b].update(support_u_books[u].copy())  # add itself to avoid empty tensor when build the support set\n",
    "        if b in support_b_users:\n",
    "            for _u in support_b_users[b]:  #  only include user in training set !!!!\n",
    "                cur_bs = support_u_books[_u]  # list\n",
    "                u_b_u_books[u][b].update(cur_bs)\n",
    "    \n",
    "    for b in query_u_books[u]:\n",
    "        if b in u_b_a_books[u] or b in u_b_u_books[u]:\n",
    "            print('error!!!')\n",
    "            break\n",
    "        u_b_a_books[u][b] = set([b])\n",
    "        for _a in b_authors[b]:\n",
    "            cur_bs = a_books[_a]\n",
    "            u_b_a_books[u][b].update(cur_bs)\n",
    "        \n",
    "        u_b_u_books[u][b] = set([b])\n",
    "        u_b_u_books[u][b].update(support_u_books[u].copy())  # add itself to avoid empty tensor when build the support set\n",
    "        if b in support_b_users:\n",
    "            for _u in support_b_users[b]:  #  only include user in training set !!!!\n",
    "                cur_bs = support_u_books[_u]  # list\n",
    "                u_b_u_books[u][b].update(cur_bs)\n",
    "        \n",
    "print(len(u_b_u_books), len(u_b_a_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4220it [06:09, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_ub_app = []\n",
    "    support_ubub_app = []\n",
    "    support_ubab_app = []\n",
    "        \n",
    "    for b_id in support_u_books[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UB\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_books[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UBUB\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_books[u_id][b_id])), dim=0))\n",
    "        # UBAB\n",
    "        support_ubab_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_a_books[u_id][b_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_books_y[u_id])\n",
    "\n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubab_app, open(\"{}/{}/support_ubab_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "    query_x_app = None\n",
    "    query_ub_app = []\n",
    "    query_ubub_app = []\n",
    "    query_ubab_app = []\n",
    "        \n",
    "    for b_id in query_u_books[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_books[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_books[u_id][b_id])), dim=0))\n",
    "        # UMAM\n",
    "        query_ubab_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_a_books[u_id][b_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_books_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubab_app,open(\"{}/{}/query_ubab_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "572it [00:00, 464320.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572\n",
      "572\n",
      "4792 4220\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state = 'warm_up'\n",
    "# state = 'user_cold_testing'\n",
    "# state = 'item_cold_testing'\n",
    "state = 'user_and_item_cold_testing'\n",
    "\n",
    "support_u_books = json.load(open(output_dir+state+'/support_u_books.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_books = json.load(open(output_dir+state+'/query_u_books.json','r'), object_hook=jsonKeys2int)\n",
    "support_u_books_y = json.load(open(output_dir+state+'/support_u_books_y.json','r'), object_hook=jsonKeys2int)\n",
    "query_u_books_y = json.load(open(output_dir+state+'/query_u_books_y.json','r'), object_hook=jsonKeys2int)\n",
    "if support_u_books.keys() == query_u_books.keys():\n",
    "    u_id_list = support_u_books.keys()\n",
    "print(len(u_id_list))\n",
    "\n",
    "cur_train_u_books =  train_u_books.copy()\n",
    "\n",
    "if support_u_books.keys() == query_u_books.keys():\n",
    "    u_id_list = support_u_books.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in tqdm(enumerate(u_id_list)):\n",
    "    if u_id not in cur_train_u_books:\n",
    "        cur_train_u_books[u_id] = []\n",
    "    cur_train_u_books[u_id] += support_u_books[u_id]\n",
    "\n",
    "print(len(cur_train_u_books),  len(train_u_books))\n",
    "print(len(set(train_u_id_list) & set(u_id_list)))\n",
    "\n",
    "(len(u_id_list) +  len(train_u_books) - len(set(train_u_id_list) & set(u_id_list))) == len(set(cur_train_u_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 452/572 [00:00<00:00, 4494.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_and_item_cold_testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 572/572 [00:00<00:00, 1891.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572 572\n",
      "4792 4220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "u_b_u_books = {}\n",
    "u_b_a_books= {}\n",
    "\n",
    "cur_train_b_users = reverse_dict(cur_train_u_books)\n",
    "\n",
    "for u in tqdm(u_id_list):\n",
    "    u_b_u_books[u] = {}\n",
    "    u_b_a_books[u] = {}\n",
    "    for b in support_u_books[u]:\n",
    "        u_b_a_books[u][b] = set([b])\n",
    "        for _a in b_authors[b]:\n",
    "            cur_bs = a_books[_a]\n",
    "            u_b_a_books[u][b].update(cur_bs)\n",
    "        \n",
    "        u_b_u_books[u][b] = set([b])\n",
    "        u_b_u_books[u][b].update(cur_train_u_books[u].copy())  # add itself to avoid empty tensor when build the support set\n",
    "        if b in support_b_users:\n",
    "            for _u in cur_train_b_users[b]:  #  only include user in training set !!!!\n",
    "                cur_bs = cur_train_u_books[_u]  # list\n",
    "                u_b_u_books[u][b].update(cur_bs)\n",
    "    \n",
    "    for b in query_u_books[u]:\n",
    "        if b in u_b_a_books[u] or b in u_b_u_books[u]:\n",
    "            print('error!!!')\n",
    "            break\n",
    "        u_b_a_books[u][b] = set([b])\n",
    "        for _a in b_authors[b]:\n",
    "            cur_bs = a_books[_a]\n",
    "            u_b_a_books[u][b].update(cur_bs)\n",
    "        \n",
    "        u_b_u_books[u][b] = set([b])\n",
    "        u_b_u_books[u][b].update(cur_train_u_books[u].copy())  # add itself to avoid empty tensor when build the support set\n",
    "        if b in support_b_users:\n",
    "            for _u in cur_train_b_users[b]:  #  only include user in training set !!!!\n",
    "                cur_bs = cur_train_u_books[_u]  # list\n",
    "                u_b_u_books[u][b].update(cur_bs)\n",
    "        \n",
    "print(len(u_b_u_books), len(u_b_a_books))\n",
    "print(len(cur_train_u_books), len(train_u_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "572it [00:04, 127.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_ub_app = []\n",
    "    support_ubub_app = []\n",
    "    support_ubab_app = []\n",
    "        \n",
    "    for b_id in support_u_books[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UB\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_train_u_books[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UBUB\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_books[u_id][b_id])), dim=0))\n",
    "        # UBAB\n",
    "        support_ubab_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_a_books[u_id][b_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_books_y[u_id])\n",
    "\n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubab_app, open(\"{}/{}/support_ubab_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "    query_x_app = None\n",
    "    query_ub_app = []\n",
    "    query_ubub_app = []\n",
    "    query_ubab_app = []\n",
    "        \n",
    "    for b_id in query_u_books[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_train_u_books[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_books[u_id][b_id])), dim=0))\n",
    "        # UMAM\n",
    "        query_ubab_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_a_books[u_id][b_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_books_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubab_app,open(\"{}/{}/query_ubab_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
