{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'original/'\n",
    "output_dir = './'\n",
    "melu_output_dir = '../../../MeLU/yelp/'\n",
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]\n",
    "\n",
    "if not os.path.exists(\"{}/meta_training/\".format(output_dir)):\n",
    "    os.mkdir(\"{}/log/\".format(output_dir))\n",
    "    for state in states:\n",
    "        os.mkdir(\"{}/{}/\".format(output_dir, state))\n",
    "        os.mkdir(\"{}/{}/\".format(melu_output_dir, state))\n",
    "        if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\n",
    "            os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302630"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ui_data = pd.read_csv(input_dir+'rating.dat', names=['user', 'item', 'rating', 'timestamp'],sep=\"\\t\", engine='python')\n",
    "len(ui_data)  # 1302630"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51670, 34259)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_list = set(ui_data.user)\n",
    "item_list = set(ui_data.item)\n",
    "len(user_list), len(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10604, 6851)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user = pd.read_csv(input_dir+'new_user.dat', names=['user'], engine='python').user.tolist()\n",
    "new_item = pd.read_csv(input_dir+'new_item.dat', names=['item'], engine='python').item.tolist()\n",
    "len(new_user), len(new_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41066, 27408)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exist_user = list(user_list - set(new_user))\n",
    "exist_item = list(item_list- set(new_item))\n",
    "len(exist_user), len(exist_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meta-training and meta-testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967291"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_training_data = ui_data[(ui_data['user'].isin(exist_user)) & (ui_data['item'].isin(exist_item))]\n",
    "len(meta_training_data)  # 967291 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96729"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warm_data = meta_training_data.sample(int(0.1*len(meta_training_data)))\n",
    "len(warm_data)  # 96729 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32145, 32145)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# warm data\n",
    "warm_x = {k: g[\"item\"].tolist() for k,g in warm_data.groupby(\"user\")}\n",
    "warm_y = {k: g[\"rating\"].tolist() for k,g in warm_data.groupby(\"user\")}\n",
    "json.dump(warm_x, open(\"{}/warm_up.json\".format(output_dir), 'w'))\n",
    "json.dump(warm_y, open(\"{}/warm_up_y.json\".format(output_dir), 'w'))\n",
    "len(warm_x), len(warm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_user_interaction = dict(warm_data.user.value_counts())\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\n",
    "len(less_100_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870562"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = meta_training_data.loc[meta_training_data.index.difference(warm_data.index)]\n",
    "len(training_data)  # 870562 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41066, 41066)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data\n",
    "training_x = {k: g[\"item\"].tolist() for k,g in training_data.groupby(\"user\")}\n",
    "training_y = {k: g[\"rating\"].tolist() for k,g in training_data.groupby(\"user\")}\n",
    "json.dump(training_x, open(\"{}/meta_training.json\".format(output_dir), 'w'))\n",
    "json.dump(training_y, open(\"{}/meta_training_y.json\".format(output_dir), 'w'))\n",
    "len(training_x), len(training_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20635"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_user_interaction = dict(training_data.user.value_counts())\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\n",
    "len(less_100_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meta-testing\n",
    "# user_cold_testing\n",
    "user_cold_data =ui_data[(ui_data['user'].isin(new_user)) & (ui_data['item'].isin(exist_item))]\n",
    "len(user_cold_data)  # 164136 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10603, 10603)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_cold_x = {k: g[\"item\"].tolist() for k,g in user_cold_data.groupby(\"user\")}\n",
    "user_cold_y = {k: g[\"rating\"].tolist() for k,g in user_cold_data.groupby(\"user\")}\n",
    "json.dump(user_cold_x, open(\"{}/user_cold_testing.json\".format(output_dir), 'w'))\n",
    "json.dump(user_cold_y, open(\"{}/user_cold_testing_y.json\".format(output_dir), 'w'))\n",
    "len(user_cold_x), len(user_cold_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4334"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_user_interaction = dict(user_cold_data.user.value_counts())\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\n",
    "len(less_100_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118467"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# item_cold_testing\n",
    "item_cold_data =ui_data[(ui_data['user'].isin(exist_user)) & (ui_data['item'].isin(new_item))]\n",
    "len(item_cold_data)  # 118467 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25578, 25578)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_cold_x = {k: g[\"item\"].tolist() for k,g in item_cold_data.groupby(\"user\")}\n",
    "item_cold_y = {k: g[\"rating\"].tolist() for k,g in item_cold_data.groupby(\"user\")}\n",
    "json.dump(item_cold_x, open(\"{}/item_cold_testing.json\".format(output_dir), 'w'))\n",
    "json.dump(item_cold_y, open(\"{}/item_cold_testing_y.json\".format(output_dir), 'w'))\n",
    "len(item_cold_x), len(item_cold_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_user_interaction = dict(item_cold_data.user.value_counts())\n",
    "less_100_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\n",
    "len(less_100_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52736"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_and_item_cold_testing\n",
    "user_item_cold_data =ui_data[(ui_data['user'].isin(new_user)) & (ui_data['item'].isin(new_item))]\n",
    "len(user_item_cold_data)  # 52736 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9655, 9655)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_cold_x = {k: g[\"item\"].tolist() for k,g in user_item_cold_data.groupby(\"user\")}\n",
    "user_item_cold_y = {k: g[\"rating\"].tolist() for k,g in user_item_cold_data.groupby(\"user\")}\n",
    "json.dump(user_item_cold_x, open(\"{}/user_and_item_cold_testing.json\".format(output_dir), 'w'))\n",
    "json.dump(user_item_cold_y, open(\"{}/user_and_item_cold_testing_y.json\".format(output_dir), 'w'))\n",
    "len(user_item_cold_x), len(user_item_cold_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_user_interaction = dict(user_item_cold_data.user.value_counts())\n",
    "less_10_user = [k for k, v in count_user_interaction.items() if 13<=v<=100]\n",
    "len(less_10_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302630"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)+len(warm_data)+len(user_cold_data)+len(item_cold_data)+len(user_item_cold_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# support set and query set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. user and item feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fans = pd.read_csv(input_dir+'user_fans.dat', names=['user','fans'], sep='\\t', engine='python')\n",
    "# user_fans\n",
    "len(user_fans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_avgrating = pd.read_csv(input_dir+'user_avgrating.dat', names=['user','avgrating'], sep='\\t', engine='python')\n",
    "len(user_avgrating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_friends = pd.read_csv(input_dir+'user_friends.dat', names=['user','friends'], sep='\\t', engine='python')\n",
    "len(user_friends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_stars =  pd.read_csv(input_dir+'item_stars.dat', names=['item','stars'], sep='\\t', engine='python')\n",
    "len(item_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_postalcode =  pd.read_csv(input_dir+'item_postalcode.dat', names=['item','postalcode'], sep='\\t', engine='python')\n",
    "len(item_postalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_city =  pd.read_csv(input_dir+'item_city.dat', names=['item','city'], sep='\\t', engine='python')\n",
    "len(item_city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63     4614\n",
       "328    4511\n",
       "108    2650\n",
       "24     1735\n",
       "151    1699\n",
       "       ... \n",
       "107       1\n",
       "75        1\n",
       "43        1\n",
       "25        1\n",
       "511       1\n",
       "Name: city, Length: 513, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_city.city.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_category =  pd.read_csv(input_dir+'item_category.dat', names=['item','category'], sep='\\t', engine='python')\n",
    "len(item_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_types = defaultdict(set)\n",
    "for index, row in item_category.iterrows():\n",
    "    types = list(map(int, row['category'].strip().split(' ')))\n",
    "    b_types[row['item']].update(types)\n",
    "b_types = dict(b_types)\n",
    "t_businesses = reverse_dict(b_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34259,\n",
       " 6509,\n",
       " 5594,\n",
       " 5399,\n",
       " 4507,\n",
       " 3710,\n",
       " 3489,\n",
       " 3164,\n",
       " 3162,\n",
       " 3120,\n",
       " 3014,\n",
       " 2994,\n",
       " 2797,\n",
       " 2613,\n",
       " 1748,\n",
       " 1712,\n",
       " 1597,\n",
       " 1511,\n",
       " 1457,\n",
       " 1447,\n",
       " 1285,\n",
       " 1277,\n",
       " 1253,\n",
       " 1171,\n",
       " 1081,\n",
       " 1054,\n",
       " 1030,\n",
       " 1023,\n",
       " 1008,\n",
       " 946,\n",
       " 927,\n",
       " 898,\n",
       " 895,\n",
       " 889,\n",
       " 856,\n",
       " 846,\n",
       " 817,\n",
       " 730,\n",
       " 686,\n",
       " 651,\n",
       " 633,\n",
       " 630,\n",
       " 627,\n",
       " 626,\n",
       " 626,\n",
       " 575,\n",
       " 553,\n",
       " 547,\n",
       " 529,\n",
       " 510,\n",
       " 504,\n",
       " 495,\n",
       " 466,\n",
       " 465,\n",
       " 438,\n",
       " 428,\n",
       " 422,\n",
       " 419,\n",
       " 378,\n",
       " 371,\n",
       " 348,\n",
       " 321,\n",
       " 313,\n",
       " 306,\n",
       " 300,\n",
       " 272,\n",
       " 259,\n",
       " 253,\n",
       " 251,\n",
       " 251,\n",
       " 244,\n",
       " 234,\n",
       " 234,\n",
       " 233,\n",
       " 213,\n",
       " 210,\n",
       " 209,\n",
       " 194,\n",
       " 194,\n",
       " 193,\n",
       " 191,\n",
       " 179,\n",
       " 169,\n",
       " 160,\n",
       " 157,\n",
       " 157,\n",
       " 155,\n",
       " 154,\n",
       " 154,\n",
       " 146,\n",
       " 144,\n",
       " 142,\n",
       " 137,\n",
       " 136,\n",
       " 136,\n",
       " 134,\n",
       " 134,\n",
       " 133,\n",
       " 133,\n",
       " 121,\n",
       " 120,\n",
       " 116,\n",
       " 111,\n",
       " 108,\n",
       " 107,\n",
       " 107,\n",
       " 105,\n",
       " 95,\n",
       " 93,\n",
       " 90,\n",
       " 85,\n",
       " 83,\n",
       " 82,\n",
       " 80,\n",
       " 79,\n",
       " 75,\n",
       " 74,\n",
       " 70,\n",
       " 69,\n",
       " 67,\n",
       " 67,\n",
       " 67,\n",
       " 64,\n",
       " 61,\n",
       " 59,\n",
       " 58,\n",
       " 58,\n",
       " 57,\n",
       " 56,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 55,\n",
       " 53,\n",
       " 53,\n",
       " 52,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 49,\n",
       " 48,\n",
       " 47,\n",
       " 47,\n",
       " 46,\n",
       " 46,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 45,\n",
       " 42,\n",
       " 41,\n",
       " 41,\n",
       " 40,\n",
       " 39,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 36,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 34,\n",
       " 34,\n",
       " 33,\n",
       " 33,\n",
       " 33,\n",
       " 31,\n",
       " 31,\n",
       " 30,\n",
       " 29,\n",
       " 29,\n",
       " 28,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 22,\n",
       " 22,\n",
       " 22,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 15,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(v) for k, v in t_businesses.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_state =  pd.read_csv(input_dir+'item_state.dat', names=['item','state'], sep='\\t', engine='python')\n",
    "len(item_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_reviewcount =  pd.read_csv(input_dir+'item_reviewcount.dat', names=['item','reviewcount'], sep='\\t', engine='python')\n",
    "len(item_reviewcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34259"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_neighbor =  pd.read_csv(input_dir+'item_neighbor.dat', names=['item','neighbor'], sep='\\t', engine='python')\n",
    "len(item_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['102', '22', '331']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(item_category[item_category['item']==1].category)[0].strip().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18174"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(item_category.category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51670/51670 [01:51<00:00, 462.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "51670"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_fea = {}\n",
    "for i in tqdm(user_list):\n",
    "    fans_idx = list(user_fans[user_fans['user']==i].fans)[0]\n",
    "    fans = torch.tensor([[fans_idx]]).long()\n",
    "    avgrating_idx = list(user_avgrating[user_avgrating['user']==i].avgrating)[0]\n",
    "    avgrating = torch.tensor([[avgrating_idx]]).long()\n",
    "    \n",
    "    user_fea[i] = torch.cat((fans, avgrating),1)\n",
    "len(user_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir+'user_feature.npy',user_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34259/34259 [03:41<00:00, 154.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(34259, 34259)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_fea_homo = {}\n",
    "item_fea_hete = {}\n",
    "for i in tqdm(item_list):\n",
    "    stars_idx = list(item_stars[item_stars['item']==i].stars)[0]\n",
    "    stars = torch.tensor([[stars_idx]]).long()\n",
    "    postalcode_idx = list(item_postalcode[item_postalcode['item']==i].postalcode)[0]\n",
    "    postalcode = torch.tensor([[postalcode_idx]]).long()\n",
    "    reviewcount_idx = list(item_reviewcount[item_reviewcount['item']==i].reviewcount)[0]\n",
    "    reviewcount = torch.tensor([[reviewcount_idx]]).long()\n",
    "    \n",
    "    city_idx = list(item_city[item_city['item']==i].city)[0]\n",
    "    city = torch.tensor([[city_idx]]).long()\n",
    "    state_idx = list(item_state[item_state['item']==i].state)[0]\n",
    "    state = torch.tensor([[state_idx]]).long()\n",
    "    \n",
    "#     category = torch.zeros(1, 542).long()\n",
    "#     categories = list(item_category[item_category['item']==i].category)[0].strip().split(' ')\n",
    "#     for c in categories:\n",
    "#         category[0, int(c)] = 1\n",
    "#     item_fea_hete[i] = torch.cat((stars, postalcode,reviewcount, category),1)\n",
    "#     item_fea_homo[i] = torch.cat((stars, postalcode, reviewcount, city, state, category), 1)\n",
    "    \n",
    "    item_fea_hete[i] = torch.cat((stars, postalcode,reviewcount),1)\n",
    "    item_fea_homo[i] = torch.cat((stars, postalcode, reviewcount, city, state), 1)\n",
    "len(item_fea_hete), len(item_fea_homo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_dir+'item_feature_hete.npy',item_fea_hete)\n",
    "np.save(output_dir+'item_feature_homo.npy',item_fea_homo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. mp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [ \"warm_up\", \"user_cold_testing\", \"item_cold_testing\", \"user_and_item_cold_testing\",\"meta_training\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import collections\n",
    "    from collections import defaultdict\n",
    "    def reverse_dict(d):\n",
    "        # {1:[a,b,c], 2:[a,f,g],...}\n",
    "        re_d = collections.defaultdict(list)\n",
    "        for k, v_list in d.items():\n",
    "            for v in v_list:\n",
    "                re_d[v].append(k)\n",
    "        return dict(re_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm._instances.clear()\n",
    "del tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cities = {k: g[\"city\"].tolist() for k,g in item_city.groupby(\"item\")}\n",
    "c_businesses = reverse_dict(b_cities)\n",
    "b_states = {k: g[\"city\"].tolist() for k,g in item_city.groupby(\"item\")}\n",
    "s_businesses = reverse_dict(b_states)\n",
    "\n",
    "b_types = defaultdict(set)\n",
    "for index, row in item_category.iterrows():\n",
    "    types = list(map(int, row['category'].strip().split(' ')))\n",
    "    b_types[row['item']].update(types)\n",
    "b_types = dict(b_types)\n",
    "t_businesses = reverse_dict(b_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34259, 34259)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(v) for k, v in b_cities.items()]), sum([len(v) for k, v in b_states.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2069/41066 [00:00<00:01, 20680.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41066/41066 [00:01<00:00, 23166.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20635, 20635, 20635, 20635, 20635, 20635)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get UM in support set and query set \n",
    "state = \"meta_training\"\n",
    "print(state)\n",
    "u_businesses = training_x\n",
    "u_businesses_y = training_y\n",
    "\n",
    "support_u_businesses = {}\n",
    "support_u_businesses_y = {}\n",
    "query_u_businesses = {}\n",
    "query_u_businesses_y = {}\n",
    "train_u_businesses = {}\n",
    "train_u_businesses_y = {}\n",
    "\n",
    "for u_id in tqdm(u_businesses):  # each task contains support set and query set\n",
    "    seen_movie_len = len(u_businesses[u_id])\n",
    "    indices = list(range(seen_movie_len))\n",
    "    if seen_movie_len < 13 or seen_movie_len > 100:\n",
    "        continue\n",
    "    \n",
    "    support_u_businesses[u_id] = []\n",
    "    support_u_businesses_y[u_id] = []\n",
    "    query_u_businesses[u_id] = []\n",
    "    query_u_businesses_y[u_id] = []\n",
    "    \n",
    "    train_u_businesses[u_id]  = []\n",
    "    train_u_businesses_y[u_id] = []\n",
    "    \n",
    "    random.shuffle(indices)\n",
    "    tmp_movies = np.array(u_businesses[u_id])\n",
    "    tmp_y = np.array(u_businesses_y[u_id])\n",
    "    \n",
    "    support_u_businesses[u_id] += list(map(int, tmp_movies[indices[:-10]]))\n",
    "    support_u_businesses_y[u_id] += list(map(int, tmp_y[indices[:-10]]))\n",
    "    query_u_businesses[u_id] += list(map(int, tmp_movies[indices[-10:]]))\n",
    "    query_u_businesses_y[u_id] += list(map(int, tmp_y[indices[-10:]]))\n",
    "    \n",
    "    train_u_businesses[u_id] += u_businesses[u_id]\n",
    "    train_u_businesses_y[u_id] += u_businesses_y[u_id]\n",
    "    \n",
    "\n",
    "json.dump(support_u_businesses, open(output_dir+state+'/support_u_businesses.json','w'))\n",
    "json.dump(support_u_businesses_y, open(output_dir+state+'/support_u_businesses_y.json','w'))\n",
    "json.dump(query_u_businesses, open(output_dir+state+'/query_u_businesses.json','w'))\n",
    "json.dump(query_u_businesses_y, open(output_dir+state+'/query_u_businesses_y.json','w'))\n",
    "len(support_u_businesses), len(support_u_businesses_y), len(query_u_businesses), len(query_u_businesses_y), len(train_u_businesses), len(train_u_businesses_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20635/20635 [06:21<00:00, 54.11it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20635\n",
      "20635\n",
      "write done!\n"
     ]
    }
   ],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "# u_b_u_businesses = {}\n",
    "u_b_c_businesses = {}\n",
    "u_b_s_businesses = {}\n",
    "# u_b_t_businesses = {}\n",
    "\n",
    "support_b_users = reverse_dict(support_u_businesses)\n",
    "\n",
    "for u, bs in tqdm(train_u_businesses.items()):\n",
    "#     u_b_u_businesses[u] = []\n",
    "    u_b_c_businesses[u] = []\n",
    "    u_b_s_businesses[u] = []\n",
    "#     u_b_t_businesses[u] = []\n",
    "    for b in bs:    \n",
    "#         cur_bs = set([b])\n",
    "#         if b in support_b_users:  # for meta_training, only support set can be seen!!!\n",
    "#             for _u in support_b_users[b]:  #  only include user in training set !!!!\n",
    "#                 cur_bs.update(support_u_businesses[_u])  # list        \n",
    "#         u_b_u_businesses[u].append(list(cur_bs))\n",
    "        \n",
    "        cur_bs = set()\n",
    "        for _c in b_cities[b]:\n",
    "            cur_bs.update(c_businesses[_c])\n",
    "        u_b_c_businesses[u].append(list(cur_bs))\n",
    "        \n",
    "        cur_bs = set()\n",
    "        for _s in b_states[b]:\n",
    "            cur_bs.update(s_businesses[_s])\n",
    "        u_b_s_businesses[u].append(list(cur_bs))\n",
    "        \n",
    "#         cur_bs = set()\n",
    "#         for _t in b_types[b]:\n",
    "#             cur_bs.update(t_businesses[_t])\n",
    "#         u_b_t_businesses[u].append(list(cur_bs))\n",
    "\n",
    "# print(len(u_b_u_businesses))\n",
    "print(len(u_b_c_businesses))\n",
    "print(len(u_b_s_businesses))\n",
    "# print(len(u_b_t_businesses))\n",
    "\n",
    "# json.dump(u_b_u_businesses, open(output_dir+state+'/u_b_u_businesses.json','w'))\n",
    "json.dump(u_b_c_businesses, open(output_dir+state+'/u_b_c_businesses.json','w')) \n",
    "json.dump(u_b_s_businesses, open(output_dir+state+'/u_b_s_businesses.json','w')) \n",
    "# json.dump(u_b_t_businesses, open(output_dir+state+'/u_b_t_businesses.json','w'))\n",
    "print('write done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_u_businesses[4]) == len(u_b_u_businesses[4]), len(train_u_businesses[4]) == len(u_b_c_businesses[4]), len(train_u_businesses[4]) == len(u_b_s_businesses[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [55:20,  7.76s/it]\n",
      "  1%|▏         | 433/34259 [00:10<14:18, 39.39it/s] \n"
     ]
    }
   ],
   "source": [
    "tqdm._instances.clear()\n",
    "del tqdm\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "428it [40:27,  2.81s/it]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c3572ebecee0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubub_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubub_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubcb_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubcb_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_ubsb_app\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_ubsb_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;31m# data for MeLU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x_app_melu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}/support_x_{}.pkl\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelu_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "if support_u_businesses.keys() == query_u_businesses.keys():\n",
    "    u_id_list = support_u_businesses.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_x_app_melu = None\n",
    "    support_ub_app = []\n",
    "    support_ubub_app = []\n",
    "    support_ubcb_app = []\n",
    "    support_ubsb_app = []\n",
    "        \n",
    "    for index1, b_id in enumerate(support_u_businesses[u_id]):\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "            support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "            support_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "        # meta-paths\n",
    "        # UB\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UBUB\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index1])), dim=0))\n",
    "        # UBCB\n",
    "        support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index1])), dim=0))\n",
    "        # UBSB\n",
    "        support_ubsb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_s_businesses[u_id][index1])), dim=0))\n",
    "#         # UBTB\n",
    "#         support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][b_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_businesses_y[u_id])\n",
    "    \n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubsb_app, open(\"{}/{}/support_ubsb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    # data for MeLU\n",
    "    pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "    query_x_app = None\n",
    "    query_x_app_melu = None\n",
    "    query_ub_app = []\n",
    "    query_ubub_app = []\n",
    "    query_ubcb_app = []\n",
    "    query_ubsb_app = []\n",
    "        \n",
    "    for index2, b_id in enumerate(query_u_businesses[u_id]):\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "            query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "            query_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "        # meta-paths\n",
    "        # UB\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], support_u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UBUB\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index2])), dim=0))\n",
    "        # UBCB\n",
    "        query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index2])), dim=0))\n",
    "        # UBSB\n",
    "        query_ubsb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_s_businesses[u_id][index2])), dim=0))\n",
    "#         # UBTB\n",
    "#         query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][m_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_businesses_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubsb_app,open(\"{}/{}/query_ubsb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    # data for MeLU\n",
    "    pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "\n",
    "    with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "        for i, b_id in enumerate(support_u_businesses[u_id]):\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, support_u_businesses_y[u_id][i]))\n",
    "                \n",
    "    with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "        for i, b_id in enumerate(query_u_businesses[u_id]):\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  query_u_businesses_y[u_id][i]))\n",
    "            \n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get UM in support set and query set \n",
    "# state = \"warm_up\"\n",
    "# print(state)\n",
    "# u_businesses = warm_x\n",
    "# u_businesses_y = warm_y\n",
    "\n",
    "# state = \"user_cold_testing\"\n",
    "# print(state)\n",
    "# u_businesses = user_cold_x\n",
    "# u_businesses_y = user_cold_y\n",
    "\n",
    "# state = \"item_cold_testing\"\n",
    "# print(state)\n",
    "# u_businesses = item_cold_x\n",
    "# u_businesses_y = item_cold_y\n",
    "\n",
    "state = \"user_and_item_cold_testing\"\n",
    "print(state)\n",
    "u_businesses = user_item_cold_x\n",
    "u_businesses_y = user_item_cold_y\n",
    "\n",
    "support_u_businesses = {}\n",
    "support_u_businesses_y = {}\n",
    "query_u_businesses = {}\n",
    "query_u_businesses_y = {}\n",
    "\n",
    "cur_training_u_businesses = train_u_businesses\n",
    "\n",
    "for u_id in tqdm(u_businesses):  # each task contains support set and query set\n",
    "    seen_movie_len = len(u_businesses[u_id])\n",
    "    indices = list(range(seen_movie_len))\n",
    "    if seen_movie_len < 13 or seen_movie_len > 100:\n",
    "        continue\n",
    "    \n",
    "    support_u_businesses[u_id] = []\n",
    "    support_u_businesses_y[u_id] = []\n",
    "    query_u_businesses[u_id] = []\n",
    "    query_u_businesses_y[u_id] = []\n",
    "    \n",
    "    random.shuffle(indices)\n",
    "    tmp_movies = np.array(u_businesses[u_id])\n",
    "    tmp_y = np.array(u_businesses_y[u_id])\n",
    "    \n",
    "    support_u_businesses[u_id] += list(map(int, tmp_movies[indices[:-10]]))\n",
    "    support_u_businesses_y[u_id] += list(map(int, tmp_y[indices[:-10]]))\n",
    "    query_u_businesses[u_id] += list(map(int, tmp_movies[indices[-10:]]))\n",
    "    query_u_businesses_y[u_id] += list(map(int, tmp_y[indices[-10:]]))\n",
    "    \n",
    "    if u_id in cur_training_u_businesses:\n",
    "        cur_training_u_businesses[u_id] += support_u_businesses[u_id]  # based on meat-traing, add the current support set data\n",
    "    else:\n",
    "        cur_training_u_businesses[u_id] = support_u_businesses[u_id]\n",
    "    \n",
    "json.dump(support_u_businesses, open(output_dir+state+'/support_u_businesses.json','w'))\n",
    "json.dump(support_u_businesses_y, open(output_dir+state+'/support_u_businesses_y.json','w'))\n",
    "json.dump(query_u_businesses, open(output_dir+state+'/query_u_businesses.json','w'))\n",
    "json.dump(query_u_businesses_y, open(output_dir+state+'/query_u_businesses_y.json','w'))\n",
    "\n",
    "len(support_u_businesses), len(support_u_businesses_y), len(query_u_businesses), len(query_u_businesses_y), len(cur_training_u_businesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mp data \n",
    "print(state)\n",
    "\n",
    "u_m_u_movies = {}\n",
    "u_m_a_movies = {}\n",
    "u_m_d_movies = {}\n",
    "\n",
    "cur_training_m_users = reverse_dict(cur_training_u_movies)\n",
    "\n",
    "if support_u_movies.keys() == query_u_movies.keys():\n",
    "    u_id_list = support_u_movies.keys()\n",
    "print(len(u_id_list))\n",
    "\n",
    "for u in tqdm(u_id_list):\n",
    "    u_m_u_movies[u] = {}\n",
    "    u_m_a_movies[u] = {}\n",
    "    u_m_d_movies[u] = {}\n",
    "    for m in support_u_movies[u]:\n",
    "        u_m_u_movies[u][m] = [m]   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_a_movies[u][m] = []   \n",
    "        u_m_d_movies[u][m] = []  \n",
    "        \n",
    "        if m in cur_training_m_users:  # include users in meta-training  and users  in current support set\n",
    "            for _u in cur_training_m_users[m]:  \n",
    "                cur_ms = cur_training_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].extend(cur_ms)\n",
    "        u_m_u_movies[u][m] = list(set(u_m_u_movies[u][m]))\n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].extend(cur_ms)\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].extend(cur_ms)\n",
    "    \n",
    "    for m in query_u_movies[u]:\n",
    "        u_m_u_movies[u][m] = [m]   # add itself to avoid empty tensor when build the support set\n",
    "        u_m_a_movies[u][m] = []   \n",
    "        u_m_d_movies[u][m] = []  \n",
    "        \n",
    "        if m in cur_training_m_users:  # include users in meta-training  and users  in current support set\n",
    "            for _u in cur_training_m_users[m]:  \n",
    "                cur_ms = cur_training_u_movies[_u]  # list\n",
    "                u_m_u_movies[u][m].extend(cur_ms)\n",
    "        u_m_u_movies[u][m] = list(set(u_m_u_movies[u][m]))       \n",
    "        for _a in m_actors[m]:\n",
    "            cur_ms = a_movies[_a]\n",
    "            u_m_a_movies[u][m].extend(cur_ms)\n",
    "        for _d in m_directors[m]:\n",
    "            cur_ms = d_movies[_d]\n",
    "            u_m_d_movies[u][m].extend(cur_ms)\n",
    "             \n",
    "print(len(u_m_u_movies), len(u_m_a_movies), len(u_m_d_movies))\n",
    "\n",
    "json.dump(u_m_u_movies, open(output_dir+state+'/u_m_u_movies.json','w'))\n",
    "json.dump(u_m_a_movies, open(output_dir+state+'/u_m_a_movies.json','w'))\n",
    "json.dump(u_m_d_movies, open(output_dir+state+'/u_m_d_movies.json','w')) \n",
    "print('write done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if support_u_businesses.keys() == query_u_businesses.keys():\n",
    "    u_id_list = support_u_businesses.keys()\n",
    "print(len(u_id_list))\n",
    "for idx, u_id in  tqdm(enumerate(u_id_list)):\n",
    "    support_x_app = None\n",
    "    support_x_app_melu = None\n",
    "    support_ub_app = []\n",
    "    support_ubub_app = []\n",
    "    support_ubcb_app = []\n",
    "    support_ubtb_app = []\n",
    "        \n",
    "    for b_id in support_u_businesses[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "            support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\n",
    "        except:\n",
    "            support_x_app = tmp_x_converted\n",
    "            support_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_training_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][b_id])), dim=0))\n",
    "        # UMAM\n",
    "        support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][b_id])), dim=0))\n",
    "        # UMDM\n",
    "        support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][b_id])), dim=0))\n",
    "        \n",
    "    support_y_app = torch.FloatTensor(support_u_businesses_y[u_id])\n",
    "    \n",
    "    pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_ubtb_app, open(\"{}/{}/support_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    # data for MeLU\n",
    "    pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    \n",
    "    query_x_app = None\n",
    "    query_x_app_melu = None\n",
    "    query_ub_app = []\n",
    "    query_ubub_app = []\n",
    "    query_ubcb_app = []\n",
    "    query_ubtb_app = []\n",
    "        \n",
    "    for b_id in query_u_businesses[u_id]:\n",
    "        tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "        tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "        try:\n",
    "            query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "            query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\n",
    "        except:\n",
    "            query_x_app = tmp_x_converted\n",
    "            query_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "        # meta-paths\n",
    "        # UM\n",
    "        query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], cur_training_u_movies[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "        # UMUM\n",
    "        query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][m_id])), dim=0))\n",
    "        # UMAM\n",
    "        query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][m_id])), dim=0))\n",
    "        # UMDM\n",
    "        query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][m_id])), dim=0))\n",
    "        \n",
    "    query_y_app = torch.FloatTensor(query_u_businesses_y[u_id])\n",
    "    \n",
    "    pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ub_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_ubtb_app,open(\"{}/{}/query_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "    # data for MeLU\n",
    "    pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "    pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "\n",
    "    with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "        for i, b_id in enumerate(support_u_businesses[u_id]):\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, support_u_businesses_y[u_id][i]))\n",
    "                \n",
    "    with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "        for i, b_id in enumerate(query_u_businesses[u_id]):\n",
    "            f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  query_u_businesses_y[u_id][i]))\n",
    "            \n",
    "print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # state = \"user_and_item_cold_testing\"\n",
    "# # u_businesses = user_item_cold_x\n",
    "# # u_businesses_y = user_item_cold_y\n",
    "\n",
    "# # state = \"user_cold_testing\"\n",
    "# # u_businesses = user_cold_x\n",
    "# # u_businesses_y = user_cold_y\n",
    "\n",
    "# # state = \"item_cold_testing\"\n",
    "# # u_businesses = item_cold_x\n",
    "# # u_businesses_y = item_cold_y\n",
    "\n",
    "# # state = \"warm_up\"\n",
    "# # u_businesses = warm_up_x\n",
    "# # u_businesses_y = warm_up_y\n",
    "\n",
    "# state = 'meta_training'\n",
    "# u_businesses = training_x\n",
    "# u_businesses_y = training_y\n",
    "\n",
    "# u_b_u_businesses = {}\n",
    "# u_b_c_businesses = {}\n",
    "# u_b_t_businesses = {}\n",
    "\n",
    "# for u, bs in tqdm(u_businesses.items()):\n",
    "#     u_b_u_businesses[u] = []\n",
    "#     u_b_c_businesses[u] = []\n",
    "#     u_b_t_businesses[u] = []\n",
    "#     for b in bs:\n",
    "#         if b in train_b_users:\n",
    "#             for _u in train_b_users[b]:  #  include user in training set !!!!\n",
    "#                 u_b_u_businesses[u].append(training_x[_u])\n",
    "#         else:\n",
    "#             u_b_u_businesses[u].append([b])  # add itself to avoid empty tensor when build the support set\n",
    "#         for _c in b_cities[b]:\n",
    "#             u_b_c_businesses[u].append(c_businesses[_c])\n",
    "#         for _t in b_types[b]:\n",
    "#             u_b_t_businesses[u].append(t_businesses[_t])\n",
    "        \n",
    "# print(len(u_b_u_businesses), len(u_b_c_businesses), len(u_b_t_businesses))\n",
    "    \n",
    "# np.save(output_dir+state+'/u_b_u_businesses.npy',u_b_u_businesses)\n",
    "# np.save(output_dir+state+'/u_b_c_businesses.npy',u_b_c_businesses)\n",
    "# np.save(output_dir+state+'/u_b_t_businesses.npy',u_b_t_businesses)\n",
    "# # json.dump(u_b_u_businesses, open(output_dir+state+'/u_b_u_businesses.json', 'w'))\n",
    "# # json.dump(u_b_c_businesses, open(output_dir+state+'/u_b_c_businesses.json', 'w'))\n",
    "# # json.dump(u_b_t_businesses, open(output_dir+state+'/u_b_t_businesses.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if not os.path.exists(\"{}/log/\".format(output_dir)):\n",
    "#         os.mkdir(\"{}/log/\".format(output_dir))\n",
    "#     if not os.path.exists(\"{}/{}/{}\".format(output_dir, \"log\", state)):\n",
    "#         os.mkdir(\"{}/{}/{}\".format(output_dir, \"log\", state))\n",
    "    \n",
    "#     print(state)\n",
    "#     print(len(u_businesses), len(u_b_u_businesses), len(u_b_c_businesses), len(u_b_t_businesses))\n",
    "#     idx = 0\n",
    "#     for _, u_id in tqdm(enumerate(u_businesses.keys())):  # each task contains support set and query set\n",
    "#         seen_business_len = len(u_businesses[u_id])\n",
    "#         indices = list(range(seen_business_len))\n",
    "        \n",
    "#         if seen_business_len < 13 or seen_business_len > 100:\n",
    "#             continue\n",
    "            \n",
    "#         random.shuffle(indices)\n",
    "#         tmp_businesses = np.array(u_businesses[u_id])\n",
    "#         tmp_y = np.array(u_businesses_y[u_id])\n",
    "\n",
    "#         support_x_app = None\n",
    "#         support_x_app_melu = None\n",
    "#         support_ub_app = []\n",
    "#         support_ubub_app = []\n",
    "#         support_ubcb_app = []\n",
    "#         support_ubtb_app = []\n",
    "#         for index1, b_id in enumerate(tmp_businesses[indices[:-10]]):\n",
    "#             u_id = int(u_id)\n",
    "#             b_id = int(b_id)\n",
    "#             tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "#             tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "#             try:\n",
    "#                 support_x_app = torch.cat((support_x_app, tmp_x_converted), 0)\n",
    "#                 support_x_app_melu = torch.cat((support_x_app_melu, tmp_x_converted_melu), 0)\n",
    "#             except:\n",
    "#                 support_x_app = tmp_x_converted\n",
    "#                 support_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "#             # meta-paths\n",
    "#             # UM\n",
    "#             support_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_businesses[u_id])), dim=0))  # each element: (#neighbor, 26=1+25)\n",
    "#             # UMUM\n",
    "#             support_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index1])), dim=0))\n",
    "#             # UMAM\n",
    "#             support_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index1])), dim=0))\n",
    "#             # UMDM\n",
    "#             support_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][index1])), dim=0))\n",
    "        \n",
    "#         support_y_app = torch.FloatTensor(tmp_y[indices[:-10]])\n",
    "\n",
    "#         query_x_app = None\n",
    "#         query_x_app_melu = None\n",
    "#         query_ub_app = []\n",
    "#         query_ubub_app = []\n",
    "#         query_ubcb_app = []\n",
    "#         query_ubtb_app = []\n",
    "#         for index2, b_id in enumerate(tmp_businesses[indices[-10:]]):\n",
    "#             u_id = int(u_id)\n",
    "#             b_id = int(b_id)\n",
    "#             tmp_x_converted = torch.cat((item_fea_hete[b_id], user_fea[u_id]), 1)\n",
    "#             tmp_x_converted_melu = torch.cat((item_fea_homo[b_id], user_fea[u_id]), 1)\n",
    "#             try:\n",
    "#                 query_x_app = torch.cat((query_x_app, tmp_x_converted), 0)\n",
    "#                 query_x_app_melu = torch.cat((query_x_app_melu, tmp_x_converted_melu), 0)\n",
    "#             except:\n",
    "#                 query_x_app = tmp_x_converted\n",
    "#                 query_x_app_melu = tmp_x_converted_melu\n",
    "\n",
    "#             # meta-paths\n",
    "#             # UM\n",
    "#             query_ub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_businesses[u_id])), dim=0))\n",
    "#             # UMUM\n",
    "#             query_ubub_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_u_businesses[u_id][index2])), dim=0))\n",
    "#             # UMAM\n",
    "#             query_ubcb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_c_businesses[u_id][index2])), dim=0))\n",
    "#             # UMDM\n",
    "#             query_ubtb_app.append(torch.cat(list(map(lambda x: item_fea_hete[x], u_b_t_businesses[u_id][index2])), dim=0))\n",
    "\n",
    "#         query_y_app = torch.FloatTensor(tmp_y[indices[-10:]])\n",
    "\n",
    "#         pickle.dump(support_x_app, open(\"{}/{}/support_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ub_app, open(\"{}/{}/support_ub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubub_app, open(\"{}/{}/support_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubcb_app, open(\"{}/{}/support_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_ubtb_app, open(\"{}/{}/support_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "        \n",
    "#         pickle.dump(query_x_app, open(\"{}/{}/query_x_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ub_app, open(\"{}/{}/query_um_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubub_app,open(\"{}/{}/query_ubub_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubcb_app,open(\"{}/{}/query_ubcb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_ubtb_app,open(\"{}/{}/query_ubtb_{}.pkl\".format(output_dir, state, idx), \"wb\"))\n",
    "        \n",
    "#         # data for MeLU\n",
    "#         pickle.dump(support_x_app_melu, open(\"{}/{}/support_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(support_y_app, open(\"{}/{}/support_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_x_app_melu, open(\"{}/{}/query_x_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "#         pickle.dump(query_y_app, open(\"{}/{}/query_y_{}.pkl\".format(melu_output_dir, state, idx), \"wb\"))\n",
    "\n",
    "#         with open(\"{}/log/{}/supp_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "#             for i, b_id in enumerate(tmp_businesses[indices[:-10]]):\n",
    "#                 f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id, tmp_y[indices[:-10]][i]))\n",
    "#         with open(\"{}/log/{}/query_x_{}_u_i_ids.txt\".format(output_dir, state, idx), \"w\") as f:\n",
    "#             for i, b_id in enumerate(tmp_businesses[indices[-10:]]):\n",
    "#                 f.write(\"{}\\t{}\\t{}\\n\".format(u_id, b_id,  tmp_y[indices[-10:]][i]))\n",
    "#         idx += 1\n",
    "        \n",
    "#     print(idx)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
